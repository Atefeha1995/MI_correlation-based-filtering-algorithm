





# Packages
```{r}
#library(tidyverse)
library(ggplot2)
library(caret)
library(randomForest)
library(mpmi)
library(dataPreparation)
library(readr)
library(tidymodels)
library(recipes)
library(rsample)
library(parsnip)
library(workflows)
library(glmnet)
```




# Read the X_train data
```{r}
X_train = read.csv(file = 'C:/Users/anisi/OneDrive/Documents/Stat 585/MI_correlation-based-filtering-algorithm/X_train_data_stat585.csv', header = FALSE)


head(X_train)
```



# Read the y_train data
```{r}
y_train = read.csv(file = 'C:/Users/anisi/OneDrive/Documents/Stat 585/MI_correlation-based-filtering-algorithm/y_train_data_stat585.csv', header = FALSE)


head(y_train)
```





# Read the X_test data
```{r}
X_test = read.csv(file = 'C:/Users/anisi/OneDrive/Documents/Stat 585/MI_correlation-based-filtering-algorithm/X_test_data_stat585.csv', header = FALSE)


head(X_test)
```



# Read the y_test data
```{r}
y_test = read.csv(file = 'C:/Users/anisi/OneDrive/Documents/Stat 585/MI_correlation-based-filtering-algorithm/y_test_data_stat585.csv', header = FALSE)


head(y_test)
```






# Main MI and correlation function
```{r}
correlation_based_filtering = function(X_tarin, y_train, MI_threshold, cor_threshold, X_test){
  
  # Inputs: 
  # 1) X_train (Original X_train)
  # 2) y_train
  # 3) MI_threshold which is a threshold for the mutual information score between the features (X_train) and the response variable (y_train)
  # 4) cor_threshold which is a threshold for the linear correlation between the features (X_train)
  # 5) X_test (Original X_test)
  
  
  # Output: 
  # 1) $x1 = reduced_X_train which is the reduced X_train
  # 2) $x2 = reduced_X_test which is the reduced X_test
  # 3) $x3 = features_list which is the list of selected features
  
  
  # Get the MI scores between the features (continuous) in X_train and y_train (discrete)
  mmi_output = mmi(cts = X_train, disc = y_train)
  mmi_scores_output = mmi_output$mi
  MI_scores = as.vector(mmi_scores_output)
  
  
  # Get the indices of the features that they have MI_scores more than the MI_threshold
  True_index = which(MI_scores > MI_threshold)
  
  # Reduced the X_train by choosing the features in True_index
  reduced_X_train_one = X_train[, True_index]
  
  
  
  
  
  
  # Get the MI scores between the remained features (continuous) and the y_train (discrete)
  new_mmi_output = mmi(cts = reduced_X_train_one, disc = y_train)
  new_mmi_scores_output = new_mmi_output$mi
  final_MI_scores = as.vector(new_mmi_scores_output)
  # Sort the final_MI_scores
  inds = order(final_MI_scores, decreasing = FALSE)
  # Order the reduced X_train based on ascending order of MI scores of features with y_train
  reduced_X_train_one = reduced_X_train_one[, inds]
  
  
  
  
  
  
  # Get the names of columns in reduced_X_train_one and make a list by them
  features_list = t(t(colnames(reduced_X_train_one)))
  features_list = as.vector(features_list)
  # Get the number of the features (columns names) in the list
  num_features = length(features_list)
  
  
  
  
  
  for ( e in 1:num_features){# e is the index of the features in feature list starting from first feature
    q = e + 1   # q is the index of the features which is next (after) to the feature with index e
    while (q <= num_features){# while loop to check correlation between the pairs
      if (abs(cor(reduced_X_train_one[, e], reduced_X_train_one[, q])) > cor_threshold){
        q = q + 1 # update q
        num_features = num_features - 1 # decrease the num_features if the threshold is passed
        for (i in (q-2):num_features){
          features_list[i] = features_list[(i+1)] # Update the feature list
        }
        features_list = features_list[-(num_features + 1) ] # drop the last element in the feature list
        
      }else{
        q = q + 1 # Update q
        
      }
      
    }
    e = e + 1 # Update e
  }
  
  
  
  # Select the columns in the final feature list from the train data set
  reduced_X_train = reduced_X_train_one[, features_list] 
  
  
  # Select the columns in the final feature list from the test data set
  reduced_X_test = X_test[, features_list]
  
  # Make a list by features_list and final train and test data set
  output_filtering_list = list(x1 = reduced_X_train, x2 = reduced_X_test, x3 = features_list)
  
  return(output_filtering_list)
  
}
```



# Test the main filtering function, we should implement the above filtering function
```{r}
filtering_function_output = correlation_based_filtering(X_train, y_train, MI_threshold = 0.01, cor_threshold = 0.95, X_test)
```




# Check the output "filtering_output_function" for x1
```{r}
reduced_X_train = filtering_function_output$x1
reduced_X_train
```



# Check the output "filtering_output_function" for x2
```{r}
reduced_X_test = filtering_function_output$x2
reduced_X_test
```



# Check the output "filtering_output_function" for x3
```{r}
features_list = filtering_function_output$x3
features_list
```








# Standardization of the train and test set function. This function gets the train set, fit the standardizer to the train set, then it applies the fitted standardizer to the test set.
```{r}
scaling_train_test = function(X_train, X_test){
  
  
  # Inputs: 
  # 1) X_train (in this project X_train is the reduced_X_train)
  # 2) X_test (in this project X_test is the reduced_X_test)
  
  
  
  # Output: 
  # 1) $y1 = scaled_X_train (in this project scaled_X_train is the standardized reduced_X_train)
  # 2) $y2 = scaled_X_test (in this project scaled_X_train is the standardized reduced_X_train)
  
  

  
  
  # Prepare the scaling function which will be fitted n the train set
  scaling_fit = build_scales(data_set = X_train, cols = "auto", verbose = TRUE)
  # Get the new X_train
  scaled_X_train = fast_scale(data_set = X_train, scales = scaling_fit, verbose = TRUE)
  # Apply the scaling_fit function (which have been fitted to the train set) to the test set
  scaled_X_test = fast_scale(data_set = X_test, scales = scaling_fit, verbose = TRUE)
  
  
  
  
  # Make a list by X_train and X_test
  output_standardization_list = list(y1 = scaled_X_train, y2 = scaled_X_test)
  
  
  return(output_standardization_list)
}
```






# Test the standardization function, we should implement the above standardization function
```{r}
scaling_function_output = scaling_train_test(reduced_X_train, reduced_X_test)
```





# Check the output "scaling_function_output" for y1
```{r}
scaled_X_train = scaling_function_output$y1
scaled_X_train
```




# Check the output "scaling_function_output" for y2
```{r}
scaled_X_test = scaling_function_output$y2
scaled_X_test
```




# Add column name to the y_train
```{r}
colnames(y_train)[1] = "class"
head(y_train)
```


# Add column name to the y_test
```{r}
colnames(y_test)[1] = "class"
head(y_test)
```





# Do the cbind for the train data set
```{r}
data_train = cbind(scaled_X_train, y_train)
head(data_train)
```

# Transfer from number to factor in class
```{r}
data_train$class = as.factor(data_train$class)
```







# Do the cbind for the test data set
```{r}
data_test = cbind(scaled_X_test, y_test)
head(data_test)
```



# Transfer from number to factor in class
```{r}
data_test$class = as.factor(data_test$class)
```





# Classification part by Logistic Regression
# Define the logistic regression model with penalty and mixture hyperparameters
```{r}
#Define 
log_reg = logistic_reg(mixture = tune(), penalty = tune(), engine = "glmnet")

# Define the grid search for the hyperparameters
grid = grid_regular(mixture(), penalty(), levels = c(mixture = 4, penalty = 3))


# Define the workflow for the model
log_reg_wf = workflow() %>%
  add_model(log_reg) %>%
  add_formula(class ~ .)



# Define the resampling method for the grid search
folds = vfold_cv(data_train, v = 5)



# Tune the hyperparameters using the grid search
log_reg_tuned = tune_grid(
  log_reg_wf,
  resamples = folds,
  grid = grid,
  control = control_grid(save_pred = TRUE)
)



select_best(log_reg_tuned, metric = "roc_auc")


```


# Fit and test the model
```{r}
# Fit the model using the optimal hyperparameters
log_reg_final = logistic_reg(penalty = 1e-10, mixture = 0) %>%
                 set_engine("glmnet") %>%
                 set_mode("classification") %>%
                 fit(class~., data = data_train)

# Evaluate the model performance on the testing set
pred_class <- predict(log_reg_final,
                      new_data = data_test,
                      type = "class")
results <- data_test %>%
  select(class) %>%
  bind_cols(pred_class)

# Create confusion matrix
conf_mat(results, truth = class,
         estimate = .pred_class)
```




# Precision
```{r}
precision(results, truth = class,
          estimate = .pred_class)
```


# recall
```{r}
recall(results, truth = class,
          estimate = .pred_class)
```



# Accuracy
```{r}
accuracy(results, truth = class,
          estimate = .pred_class)
```


# Sensitivity
```{r}
sensitivity(results, truth = class,
          estimate = .pred_class)
```




# Get the coefficients greater than 0.3
```{r}
coeff <- tidy(log_reg_final) %>% 
  arrange(desc(abs(estimate))) %>% 
  filter(abs(estimate) > 0.3)
```



```{r}
coeff
```





```{r}
ggplot(coeff, aes(x = term, y = estimate, fill = term)) + geom_col() + coord_flip()

ggplot
```








# 3D_plot for the relation of MI_threshold, mean MI of the chosen features
```{r}
MI_analysis_plot = function(MI_threshold_LB,MI_threshold_UB,MI_threshold_step,X_train,y_train,X_test, cor_threshold){
  
MI_threshold_values = seq(MI_threshold_LB, MI_threshold_UB, by=MI_threshold_step)

plot_input = data.frame(matrix(ncol = 3, nrow = ((MI_threshold_UB - MI_threshold_LB)/MI_threshold_step) + 1))

#provide column names
colnames(plot_input) <- c('MI_threshold', 'No. features after reduction', 'Mean_MI')

output = list()

j = 1

for (i in MI_threshold_values){
     
       plot_input [j,1] = i
       output= correlation_based_filtering(X_train, y_train, i, cor_threshold, X_test)
       plot_input [j,2] = length(output$x3)
       mmi_output = mmi(cts = output$x1, disc = y_train)
       plot_input [j,3] = mean(as.vector(mmi_output$mi))
       j= j + 1
   }
  
return (plot_input)  
  
}
```



```{r, warning=FALSE}
test_plot = MI_analysis_plot(0.001, 0.07, 0.01, X_train, y_train, X_test, 0.95)
test_plot
```

#Create the plot
```{r}
library("scatterplot3d")
scatterplot3d(test_plot[,1:3], angle = 45,
              main="3D Scatter Plot",
              xlab = "MI_threshold",
              ylab = "No. features after reduction",
              zlab = "Mean_MI",
              pch = 16,
              color = "#56B4E9",
              type = "h")
```



```{r}
library(plotly)

fig <- plot_ly(test_plot, x = test_plot$MI_threshold, y = test_plot$`No. features after reduction`, z = test_plot$Mean_MI, colors = c('#0C4B8E'))
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = 'MI_threshold'),
                     yaxis = list(title = 'No. features'),
                     zaxis = list(title = 'Mean_MI')))

fig
```

# 3D_plot for the relation of MI_threshold, cor_threshold and the number of chosen features
```{r}
MI_cor_plot = function(MI_threshold_LB,MI_threshold_UB,MI_threshold_step,cor_threshold_LB,cor_threshold_UB,cor_threshold_step,X_train,y_train,X_test){
  
cor_threshold_values = seq(cor_threshold_LB, cor_threshold_UB, by=cor_threshold_step)
MI_threshold_values = seq(MI_threshold_LB, MI_threshold_UB, by=MI_threshold_step)

plot_input = data.frame(matrix(ncol = 3, nrow = (length(cor_threshold_values)*length(MI_threshold_values))))

#provide column names
colnames(plot_input) <- c('cor_threshold', 'MI_threshold','No. features after reduction')

output = list()

j = 1

for (i in cor_threshold_values){
  
    for (f in MI_threshold_values){
     
       plot_input [j,1] = i
       plot_input [j,2] = f
       output= correlation_based_filtering(X_train, y_train, f, i, X_test)
       plot_input [j,3] = length(output$x3)
       j= j + 1
    }
}
  
return (plot_input)  
  
}
```


```{r, warning=FALSE}
test_plot_2 = MI_cor_plot(0.01, 0.02, 0.01,0.9,0.95,0.05, X_train, y_train, X_test)
test_plot_2
```
















